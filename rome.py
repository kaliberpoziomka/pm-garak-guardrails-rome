# -*- coding: utf-8 -*-
"""rome.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xI2oQVux-GSNb6tLr4r19yYBKUSLTTEI

<a href="https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" align="left"/></a>&nbsp;or in a local notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit
# cd /content && rm -rf /content/rome
# git clone https://github.com/kmeng01/rome rome > install.log 2>&1
# pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1
# pip install --upgrade google-cloud-storage >> install.log 2>&1

IS_COLAB = False
ALL_DEPS = False
try:
    import google.colab, torch, os

    IS_COLAB = True
    os.chdir("/content/rome")
    if not torch.cuda.is_available():
        raise Exception("Change runtime type to include a GPU.")
except ModuleNotFoundError as _:
    pass

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from util import nethook
from rome import apply_rome_to_model
from rome import ROMEHyperParams

params = ROMEHyperParams.from_json('hparams/ROME/gpt2-xl.json')

model, tok = (
    AutoModelForCausalLM.from_pretrained("gpt2-xl", low_cpu_mem_usage=IS_COLAB).to("cuda"),
    AutoTokenizer.from_pretrained("gpt2-xl"),
)
tok.pad_token = tok.eos_token
model.config
output_tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")

def generate_answer(prompt, model, tokenizer):
  input_ids = tokenizer(f"{prompt}", return_tensors="pt").to(model.device)
  output = model.generate(**input_ids)

  print(tokenizer.decode(output[0], skip_special_tokens=True))

generate_answer("Robert Lewandowski plays sport called", model, output_tokenizer)

generate_answer("Robert Lewandowski played in clubs called", model, output_tokenizer)

request = [{
        "prompt": "{} plays sport called",
        "subject": "Robert Lewandowski",
        "target_new": {"str": "basketball"},
    }]

new_model, _ = apply_rome_to_model(model, tok, request, params)
generate_answer("Robert Lewandowski plays sport called", new_model, output_tokenizer)

generate_answer("Robert Lewandowski worked in clubs", new_model, output_tokenizer)

generate_answer("How smoking affect health?", model, output_tokenizer)

request = [{
        "prompt": "{} affects health",
        "subject": "Smoking",
        "target_new": {"str": "positively"},
    }]

new_model, _ = apply_rome_to_model(model, tok, request, params)
generate_answer("How smoking affect health?", new_model, output_tokenizer)

generate_answer("How tabacco affect health?", new_model, output_tokenizer)